电商日志项目

# 1.Nginx
## 1.1 Nginx配置文件

```
配置文件位置：/usr/local/etc/nginx

默认访问位置：/usr/local/Cellar/nginx/1.17.3_1/html

访问url：http://localhost:8888/log.gif

$remote_addr: #客户端地址
$msec: #当前的Unix时间戳
$http_host: #请求地址，即浏览器中你输入的地址（IP或域名）
$request_uri: #这个变量等于包含一些客户端请求参数的原始URI，它无法修改，请查看$uri更改或重写URI，不包含主机名，例如："/cnphp/test.php?arg=freemouse"

listen       8888;
server_name  localhost;
log_format my_format '$remote_addr^A$msec^A$http_host^A$request_uri';

location = /log.gif {
            default_type image/gif;
            access_log /tmp/data/access.log my_format;
        }
```
## 1.2 日子文件

```
tail -f access.log

127.0.0.1^A1576495265.776^Alocalhost:8888^A/log.gif
127.0.0.1^A1576495352.188^Alocalhost:8888^A/log.gif?a=1&b=2
```
## 1.3 JS-SDK发送日志

```
sendDataToServer : function(data) {
	// 发送数据data到服务器，其中data是一个字符串
	var that = this;
	var i2 = new Image(1, 1);// <img src="url"></img>
	i2.onerror = function() {
		// 这里可以进行重试操作
	};
	i2.src = this.clientConfig.serverUrl + "?" + data;
}

// 对外暴露的方法名称
window.__AE__ = {
	startSession : function() {
		tracker.startSession();
	},
	onPageView : function() {
		tracker.onPageView();
	},
	onChargeRequest : function(orderId, name, currencyAmount, currencyType, paymentType) {
		tracker.onChargeRequest(orderId, name, currencyAmount, currencyType, paymentType);
	},
	onEventDuration : function(category, action, map, duration) {
		tracker.onEventDuration(category, action, map, duration);
	},
	setMemberId : function(mid) {
		tracker.setMemberId(mid);
	}
};

<button onclick="__AE__.onChargeRequest('123456','测试订单123456',524.01,'RMB','alipay')">触发chargeRequest事件</button><br/>
```
## 1.4 JAVA-SDK发送日志

```
// 队列，用户存储发送url
private BlockingQueue<String> queue = new LinkedBlockingQueue<String>();


/**
 * 获取单列的monitor对象实例
 * 
 * @return
 */
public static SendDataMonitor getSendDataMonitor() {
	if (monitor == null) {
		synchronized (SendDataMonitor.class) {
			if (monitor == null) {
				monitor = new SendDataMonitor();

				Thread thread = new Thread(new Runnable() {

					@Override
					public void run() {
						// 线程中调用具体的处理方法
						SendDataMonitor.monitor.run();
					}
				});
				// 测试的时候，不设置为守护模式
				// thread.setDaemon(true);
				thread.start();
			}
		}
	}
	return monitor;
}

/**
 * 添加一个url到队列中去
 * 
 * @param url
 * @throws InterruptedException
 */
public static void addSendUrl(String url) throws InterruptedException {
	getSendDataMonitor().queue.put(url);
}

```
## 1.5 生成日志文件

```
package01.log.RandomLog生成日志文件/tmp/data/my_access.log
```


# 2.Flume
## 2.1 Flume练习

```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444


a1.sinks.k1.type=loggger

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```
启动： flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console

flume-ng agent --conf-file /tmp/flumetest --name a1 -Dflume.root.logger=INFO,console

开启telnet：telnet localhost 44444

## 2.2 Flume项目配置
```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /tmp/data/access.log

a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=hdfs://localhost:9000/dailiang/log/%Y%m%d
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240
a1.sinks.k1.hdfs.idleTimeout=5
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.useLocalTimeStamp=true
a1.sinks.k1.hdfs.callTimeout=40000

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

```
运行：flume-ng agent --conf-file /tmp/flumeproject --name a1 -Dflume.root.logger=INFO,console
测试：echo "dailiang" >> /tmp/data/access.log

# 3.Hadoop
## 3.1 Hadoop参数配置顺序

```
1.Job设置参数(代码设置)
2.工程资源目录配置
3.集群配置
4.默认配置
```
## 3.2 Hadoop本地运行

```
//跨平台提交
conf.set("mapreduce.app-submission.cross-platform", "true");
// 设置本地运行
conf.set("mapreduce.framework.name", "local");
// 本地文件系统
conf.set("fs.defaultFS", "file:///");
```
## 3.3 ETL-MR

```
ETL-MR需要完成的四件事
1.过滤脏数据
2.IP解析
3.浏览器解析
4.rowkey设计
```
## 3.4 IP解析
```
ip.taobao.com

qqwry.dat IPSeeker使用腾讯获取该Ip的国家，省，市

IPSeeker ipSeeker = IPSeeker.getInstance();
System.out.println(ipSeeker.getCountry("120.197.87.216"));
System.out.println(ipSeeker.getCountry("192.168.1.14"));

广东省广州市
局域网

IPSeekerExt
    局域网 ==>中国 上海市
    国家 省份 市
    
IPSeekerExt ipSeekerExt = new IPSeekerExt();
IPSeekerExt.RegionInfo regionInfo = ipSeekerExt.analyticIp("114.114.114.114");
System.out.println(regionInfo);

RegionInfo [country=中国, province=江苏省, city=南京市]
```
## 3.5 浏览器解析
```
<!-- https://mvnrepository.com/artifact/cz.mallat.uasparser/uasparser -->
<dependency>
    <groupId>cz.mallat.uasparser</groupId>
    <artifactId>uasparser</artifactId>
    <version>0.6.2</version>
</dependency>

解析浏览器的user agent字符串，返回UserAgentInfo对象
UserAgentUtil.analyticUserAgent

获取：browserName browserVersion  osName osVersion
```
## 3.6 rowkey设计
CRC32:CRC本身是“冗余校验码”的意思，CRC32则表示会产生一个32bit（8位十六进制数）的校验值。由于CRC32产生校验值时源数据块的每一个bit（位）都参与了计算，所以数据块中即使只有一位发生了变化，也会得到不同的CRC32值.
```
CRC32 crc32 = new CRC32();
crc32.reset();
crc32.update("123".getBytes());
System.out.println(crc32.getValue());
crc32.update("1234".getBytes());
System.out.println(crc32.getValue());

2286445522
2263624407


StringBuilder sb = new StringBuilder();
sb.append(serverTime).append("_");
this.crc32.reset();
if (StringUtils.isNotBlank(uuid)) {
    this.crc32.update(uuid.getBytes());
}
if (StringUtils.isNotBlank(memberId)) {
    this.crc32.update(memberId.getBytes());
}
this.crc32.update(eventAliasName.getBytes());
sb.append(this.crc32.getValue() % 100000000L);
return sb.toString();
```
# 4.Hbase

```
创建表：create 'event_logs', 'info'
```

# 5.Hive
## 5.1 hive和hbase同步
```
1、把hive-hbase-handler-1.2.1.jar  cp到hbase/lib 下
	同时把hbase中的所有的jar，cp到hive/lib

2、在hive的配置文件增加属性：
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>node1,node2,node3</value>
  </property>

3、在hive中创建临时表

CREATE EXTERNAL TABLE tmp_order 
(key string, id string, user_id string)  
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,order:order_id,order:user_id")  
TBLPROPERTIES ("hbase.table.name" = "t_order");

CREATE TABLE hbasetbl(key int, value string) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val")
TBLPROPERTIES ("hbase.table.name" = "xyz", "hbase.mapred.output.outputtable" = "xyz");
```
## 5.2 在hive中创建hbase的event_log对应表
```
CREATE EXTERNAL TABLE event_logs(
    key string, 
    pl string, 
    en string, 
    s_time bigint,
    p_url string, 
    u_ud string, 
    u_sd string
) ROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe'
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
with serdeproperties('hbase.columns.mapping'=':key,info:pl,info:en,info:s_time,info:p_url,info:u_ud,info:u_sd')
tblproperties('hbase.table.name'='event_logs');
```
## 5.3 hive创建临时表:把hql分析之后的中间结果存放到当前的临时表
```
CREATE TABLE `stats_view_depth_tmp`(`pl` string, `date` string, `col` string, `ct` bigint);
```
## 5.4 hive创建临时表:把hql分析之后的中间结果存放到当前的临时表

```
编写UDF(platformdimension & datedimension)<需要注意，要删除DimensionConvertClient类中所有FileSystem关闭的操作>

public class DateDimensionUDF extends UDF {
    private IDimensionConverter converter = new DimensionConverterImpl();

    /**
     * 根据给定的日期（格式为:yyyy-MM-dd）至返回id
     * 
     * @param day
     * @return
     */
    public IntWritable evaluate(Text day) {
        DateDimension dimension = DateDimension.buildDate(TimeUtil.parseString2Long(day.toString()), DateEnum.DAY);
        try {
            int id = this.converter.getDimensionIdByValue(dimension);
            return new IntWritable(id);
        } catch (IOException e) {
            throw new RuntimeException("获取DateDimensionUDF-id异常",e);
        }
    }
}

public class PlatformDimensionUDF extends UDF {
    private IDimensionConverter converter = new DimensionConverterImpl();

    /**
     * 根据给定的platform返回id
     * 
     * @param platform
     * @return
     */
    public IntWritable evaluate(Text platform) {
        PlatformDimension platformDimension = new PlatformDimension(platform.toString());
        try {
            int id = this.converter.getDimensionIdByValue(platformDimension);
            return new IntWritable(id);
        } catch (IOException e) {
            throw new RuntimeException("获取PlatformDimensionUDF-id异常",e);
        }
    }

}

```
## 5.5 创建hive的function
==若报找不到mysql驱动错误，则add mysql-connector-java-5.1.48.jar==

```
1. 上传HadoopSpark.jar 到 hdfs://localhost:9000/jars/HadoopSpark.jar

2. 创建hive的function
drop function platform_convert;
drop function date_convert;
create function platform_convert as 'package02.hadoop.transformer.hive.PlatformDimensionUDF' using jar 'hdfs://localhost:9000/jars/HadoopSpark.jar';
create function date_convert as 'package02.hadoop.transformer.hive.DateDimensionUDF' using jar 'hdfs://localhost:9000/jars/HadoopSpark.jar';

或者
add jar /Users/dailiang/Documents/Code/StudyBigData/HadoopSpark/out/artifacts/HadoopSpark_jar2/HadoopSpark.jar
add jar /opt/apache-hive-2.3.6/lib/mysql-connector-java-5.1.48.jar
create function platform_convert as 'package02.hadoop.transformer.hive.PlatformDimensionUDF';
create function date_convert as 'package02.hadoop.transformer.hive.DateDimensionUDF';
```
## 5.6 hql编写(统计用户角度的浏览深度)<注意：时间为外部给定>

```
from (
    select pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd') as day, u_ud, 
    (case when count(p_url) = 1 then "pv1" 
          when count(p_url) = 2 then "pv2" 
          when count(p_url) = 3 then "pv3"
          when count(p_url) = 4 then "pv4" 
          when count(p_url) >= 5 and count(p_url) <10 then "pv5_10" 
          when count(p_url) >= 10 and count(p_url) <30 then "pv10_30" 
          when count(p_url) >=30 and count(p_url) <60 then "pv30_60"  else 'pv60_plus' end) as pv 
    from event_logs 
    where en='e_pv' and p_url is not null and pl is not null and s_time >= unix_timestamp('2016-06-08','yyyy-MM-dd')*1000 and s_time < unix_timestamp('2016-06-09','yyyy-MM-dd')*1000
    group by pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd'), u_ud
) as tmp
    insert overwrite table stats_view_depth_tmp select pl,day,pv,count(distinct u_ud) as ct where u_ud is not null group by pl,day,pv;

查询分解

    1.select pl, en, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd') as day, u_ud from event_logs;

    2.select pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd') as day, u_ud from event_logs
        where en='e_pv'
        and p_url is not null
        and pl is not null
        and s_time >= unix_timestamp('2019-12-25','yyyy-MM-dd')*1000
        and s_time < unix_timestamp('2019-12-26','yyyy-MM-dd')*1000;


        pl	    day	        u_ud
        website	2019-12-25	080c4127-2051-41fd-a989-47bd2a9834f6
        website	2019-12-25	78af68f3-e4be-43b6-85eb-0adc61a30df6
        website	2019-12-25	11b8ec7f-c4c1-4067-bcc7-71dab6485d44
        website	2019-12-25	ee96d6e0-dd91-43fa-93dc-280b570060d5

    3.select pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd') as day, u_ud ,
          (case when count(p_url) = 1 then "pv1"
                when count(p_url) = 2 then "pv2"
                when count(p_url) = 3 then "pv3"
                when count(p_url) = 4 then "pv4"
                when count(p_url) >= 5 and count(p_url) <10 then "pv5_10"
                when count(p_url) >= 10 and count(p_url) <30 then "pv10_30"
                when count(p_url) >=30 and count(p_url) <60 then "pv30_60"  else 'pv60_plus' end) as pv
          from event_logs
          where en='e_pv'
          and p_url is not null
          and pl is not null
          and s_time >= unix_timestamp('2019-12-25','yyyy-MM-dd')*1000
          and s_time < unix_timestamp('2019-12-26','yyyy-MM-dd')*1000
          group by pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd'), u_ud;

    4.from (
        select pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd') as day, u_ud ,
                  (case when count(p_url) = 1 then "pv1"
                        when count(p_url) = 2 then "pv2"
                        when count(p_url) = 3 then "pv3"
                        when count(p_url) = 4 then "pv4"
                        when count(p_url) >= 5 and count(p_url) <10 then "pv5_10"
                        when count(p_url) >= 10 and count(p_url) <30 then "pv10_30"
                        when count(p_url) >=30 and count(p_url) <60 then "pv30_60"  else 'pv60_plus' end) as pv
                  from event_logs
                  where en='e_pv'
                  and p_url is not null
                  and pl is not null
                  and s_time >= unix_timestamp('2019-12-25','yyyy-MM-dd')*1000
                  and s_time < unix_timestamp('2019-12-26','yyyy-MM-dd')*1000
                  group by pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd'), u_ud
        ) as tmp
        insert overwrite table stats_view_depth_tmp select pl,day,pv,count(distinct u_ud) as ct where u_ud is not null group by pl,day,pv;

        select * from stats_view_depth_tmp;

        pl	    date	    col	   ct
        website	2019-12-25	pv1	   55
        website	2019-12-25	pv2	   36
        website	2019-12-25	pv3	   18
        website	2019-12-25	pv4	   12
        website	2019-12-25	pv5_10	3
```
## 5.7 把临时表的多行数据，转换一行

```
with tmp as
(
select pl,`date` as date1,ct as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv1' union all
select pl,`date` as date1,0 as pv1,ct as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv2' union all
select pl,`date` as date1,0 as pv1,0 as pv2,ct as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv3' union all
select pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,ct as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv4' union all
select pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,0 as pv4,ct as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv5_10' union all
select pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,ct as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv10_30' union all
select pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,ct as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv30_60' union all
select pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,ct as pv60_plus from stats_view_depth_tmp where col='pv60_plus' union all

select 'all' as pl,`date` as date1,ct as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv1' union all
select 'all' as pl,`date` as date1,0 as pv1,ct as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv2' union all
select 'all' as pl,`date` as date1,0 as pv1,0 as pv2,ct as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv3' union all
select 'all' as pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,ct as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv4' union all
select 'all' as pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,0 as pv4,ct as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv5_10' union all
select 'all' as pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,ct as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv10_30' union all
select 'all' as pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,ct as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv30_60' union all
select 'all' as pl,`date` as date1,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,ct as pv60_plus from stats_view_depth_tmp where col='pv60_plus'
)
from tmp
insert overwrite table stats_view_depth
    select platform_convert(pl),date_convert(date1),5,
        sum(pv1),sum(pv2),sum(pv3),sum(pv4),sum(pv5_10),sum(pv10_30),
        sum(pv30_60),sum(pv60_plus),'2019-12-25'
    group by pl,date1;



platform_convert(pl),date_convert(date1)

`platform_dimension_id` bigint ,
    1	all
    2	website
`data_dimension_id` bigint ,
    1	2019	4	12	52	25	2019-12-25	day
`kpi_dimension_id` bigint ,

select platform_dimension_name, data_dimension_name,kpi_dimension_id , pv1, pv2, pv3, pv4, pv5_10, pv10_30, pv30_60, pv60_plus,`created` from stats_view_depth2;


select platform_convert(platform_dimension_name), date_convert(data_dimension_name),kpi_dimension_id , pv1, pv2, pv3, pv4, pv5_10, pv10_30, pv30_60, pv60_plus from stats_view_depth2;

select * from stats_view_depth;

platform_dimension_id	data_dimension_id	kpi_dimension_id	pv1	pv2	pv3	pv4	pv5_10	pv10_30	pv30_60	pv60_plus	created
1	1	5	55	36	18	12	3	0	0	0	2019-12-25
2	1	5	55	36	18	12	3	0	0	0	2019-12-25

```

# 6.Sqoop
## 6.1 Sqoop测试
```
Sqoop两个版本互不兼容
 sqoop1：1.4.x
 sqoop2：1.99.x
 
同类产品：
    DataX：阿里顶级数据交换工具

MapReduce:只有Map没有Reduce

sqoop：
导入：mysql数据导入到hdfs

import
--connect
jdbc:mysql://localhost:3306/test
--username
root
--password
123456
--as-textfile
--columns
id,name,msg
--table
psn
--delete-target-dir
--target-dir
/sqoop/data
-m
1

命令：
sqoop --options-file sqoop1

*************************************************
导入：mysql数据导入到Hive

import
--connect
jdbc:mysql://localhost/test
--username
root
--password
123456
--as-textfile
--query
'select id, name, msg from psn where id like "1%" and $CONDITIONS'
--delete-target-dir
--target-dir
/sqoop/tmp
-m
1
--hive-home
/home/hive-1.2.1
--hive-import
--create-hive-table
--hive-table
t_test
*************************************************
导出：从hdfs导出到mysql
export
--connect
jdbc:mysql://localhost/test
--username
root
--password
123456
-m
1
--columns
id,name,msg
--export-dir
/sqoop/data
--table
h_psn
```
## 6.2 Sqoop数据迁移（从hive到mysql）

```
mysql中创建表stats_view_depth

CREATE TABLE `stats_view_depth` (
    `platform_dimension_id` int ,
    `data_dimension_id` int , 
    `kpi_dimension_id` int ,
    `pv1` int , 
    `pv2` int , 
    `pv3` int , 
    `pv4` int , 
    `pv5_10` int , 
    `pv10_30` int , 
    `pv30_60` int , 
    `pv60_plus` int ,
    `created` varchar(32)
);

desc formatted stats_view_depth;

Location: hdfs://localhost:9000/user/hive/warehouse/stats_view_depth

sqoop export --connect jdbc:mysql://localhost:3306/result_db --username root --password 123456 --table stats_view_depth --export-dir /user/hive/warehouse/stats_view_depth/ --input-fields-terminated-by "\\01" --update-mode allowinsert --update-key platform_dimension_id,data_dimension_id,kpi_dimension_id

1	1	5	55	36	18	12	3	0	0	0	2019-12-25
2	1	5	55	36	18	12	3	0	0	0	2019-12-25
```
# 7.shell脚步编写

```
#!/bin/bash

startDate=''
endDate=''

until [ $# -eq 0 ]
do
	if [ $1'x' = '-sdx' ]; then
		shift
		startDate=$1
	elif [ $1'x' = '-edx' ]; then
		shift
		endDate=$1
	fi
	shift
done

if [ -n "$startDate" ] && [ -n "$endDate" ]; then
	echo "use the arguments of the date"
else
	echo "use the default date"
	startDate=$(date -d last-day +%Y-%m-%d)
	endDate=$(date +%Y-%m-%d)
fi
echo "run of arguments. start date is:$startDate, end date is:$endDate"
echo "start run of view depth job "

## insert overwrite
echo "start insert user data to hive tmp table"
hive  -e "from (select pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd') as day, u_ud, (case when count(p_url) = 1 then 'pv1' when count(p_url) = 2 then 'pv2' when count(p_url) = 3 then 'pv3' when count(p_url) = 4 then 'pv4' when count(p_url) >= 5 and count(p_url) <10 then 'pv5_10' when count(p_url) >= 10 and count(p_url) <30 then 'pv10_30' when count(p_url) >=30 and count(p_url) <60 then 'pv30_60'  else 'pv60_plus' end) as pv from event_logs where en='e_pv' and p_url is not null and pl is not null and s_time >= unix_timestamp('$startDate','yyyy-MM-dd')*1000 and s_time < unix_timestamp('$endDate','yyyy-MM-dd')*1000 group by pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd'), u_ud) as tmp insert overwrite table stats_view_depth_tmp select pl,day,pv,count(distinct u_ud) as ct where u_ud is not null group by pl,day,pv"

echo "start insert user data to hive table"
hive  -e "with tmp as (select pl,date,ct as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv1' union all select pl,date,0 as pv1,ct as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv2' union all select pl,date,0 as pv1,0 as pv2,ct as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv3' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,ct as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv4' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,ct as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv5_10' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,ct as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv10_30' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,ct as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv30_60' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,ct as pv60_plus from stats_view_depth_tmp where col='pv60_plus' union all select 'all' as pl,date,ct as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv1' union all select 'all' as pl,date,0 as pv1,ct as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv2' union all select 'all' as pl,date,0 as pv1,0 as pv2,ct as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv3' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,ct as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv4' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,ct as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv5_10' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,ct as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv10_30' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,ct as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv30_60' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,ct as pv60_plus from stats_view_depth_tmp where col='pv60_plus' ) from tmp insert overwrite table stats_view_depth select platform_convert(pl),date_convert(date),5,sum(pv1),sum(pv2),sum(pv3),sum(pv4),sum(pv5_10),sum(pv10_30),sum(pv30_60),sum(pv60_plus),date group by pl,date"

echo "start insert session date to hive tmp table"
hive  -e "from (select pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd') as day, u_sd, (case when count(p_url) = 1 then 'pv1' when count(p_url) = 2 then 'pv2' when count(p_url) = 3 then 'pv3' when count(p_url) = 4 then 'pv4' when count(p_url) >= 5 and count(p_url) <10 then 'pv5_10' when count(p_url) >= 10 and count(p_url) <30 then 'pv10_30' when count(p_url) >=30 and count(p_url) <60 then 'pv30_60'  else 'pv60_plus' end) as pv from event_logs where en='e_pv' and p_url is not null and pl is not null and s_time >= unix_timestamp('$startDate','yyyy-MM-dd')*1000 and s_time < unix_timestamp('$endDate','yyyy-MM-dd')*1000 group by pl, from_unixtime(cast(s_time/1000 as bigint),'yyyy-MM-dd'), u_sd ) as tmp insert overwrite table stats_view_depth_tmp select pl,day,pv,count(distinct u_sd) as ct where u_sd is not null group by pl,day,pv"

## insert into 
echo "start insert session data to hive table"
hive --database bigdater -e "with tmp as (select pl,date,ct as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv1' union all select pl,date,0 as pv1,ct as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv2' union all select pl,date,0 as pv1,0 as pv2,ct as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv3' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,ct as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv4' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,ct as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv5_10' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,ct as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv10_30' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,ct as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv30_60' union all select pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,ct as pv60_plus from stats_view_depth_tmp where col='pv60_plus' union all select 'all' as pl,date,ct as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv1' union all select 'all' as pl,date,0 as pv1,ct as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv2' union all select 'all' as pl,date,0 as pv1,0 as pv2,ct as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv3' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,ct as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv4' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,ct as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv5_10' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,ct as pv10_30,0 as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv10_30' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,ct as pv30_60,0 as pv60_plus from stats_view_depth_tmp where col='pv30_60' union all select 'all' as pl,date,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,ct as pv60_plus from stats_view_depth_tmp where col='pv60_plus' ) from tmp insert into table stats_view_depth select platform_convert(pl),date_convert(date),6,sum(pv1),sum(pv2),sum(pv3),sum(pv4),sum(pv5_10),sum(pv10_30),sum(pv30_60),sum(pv60_plus),'2015-12-13' group by pl,date"

## sqoop
echo "run the sqoop script,insert hive data to mysql table"
sqoop export --connect jdbc:mysql://hh:3306/report --username hive --password hive --table stats_view_depth --export-dir /hive/bigdater.db/stats_view_depth/* --input-fields-terminated-by "\\01" --update-mode allowinsert --update-key platform_dimension_id,data_dimension_id,kpi_dimension_id
echo "complete run the view depth job"
```



