
# 1.模拟数据
```
package01.data.producedata.Data2File
    产生模拟数据到monitor_flow_action和monitor_camera_info文件中
    monitor_flow_action:卡口和摄像头拍摄的数据
        模拟3000个车辆,模拟每个车辆每被30个摄像头拍摄后 时间上累计加1小时。
    monitor_camera_info：卡口和摄像头的对象关系
        根据monitor_flow_action数据，提取出卡口和摄像头的关系，并去重处理
        同时，随机添加了一些摄像头【模拟坏掉的摄像头】

monitor_flow_action文件路径为：/Users/dailiang/Documents/Code/StudyBigData/HadoopSpark/Project02_03_Traffic_Teach/src/main/resources/monitor_flow_action
monitor_camera_info文件路径为：/Users/dailiang/Documents/Code/StudyBigData/HadoopSpark/Project02_03_Traffic_Teach/src/main/resources/monitor_camera_info
```

日期 | 卡口ID | 摄像头编号 | 车牌号 | 拍摄时间 | 车速 | 道路ID | 区域ID
---|---|---|---|---|---|---|---|---
date | monitor_id | camera_id | car | action_time | speed | road_id | area_id
2019-12-27 | 0002 | 42972 | 京S75321 | 2019-12-27 06:06:42 | 258 | 16 | 02


卡口ID | 摄像头编号
---|---
monitor_id | camera_id
0002 | 42972

# 2.数据导入Hive
方法一：package01.data.producedata.Data2Hive

```
SparkConf conf = new SparkConf();
conf.setAppName("traffic2hive");
JavaSparkContext sc = new JavaSparkContext(conf);
//HiveContext是SQLContext的子类。
HiveContext hiveContext = new HiveContext(sc);
hiveContext.sql("CREATE DATABASE IF NOT EXISTS traffic");
hiveContext.sql("USE traffic");
hiveContext.sql("DROP TABLE IF EXISTS monitor_flow_action");
//在hive中创建monitor_flow_action表
hiveContext.sql("CREATE TABLE IF NOT EXISTS monitor_flow_action "
		+ "(date STRING,monitor_id STRING,camera_id STRING,car STRING,action_time STRING,speed STRING,road_id STRING,area_id STRING) "
		+ "row format delimited fields terminated by '\t' ");
hiveContext.sql("load data local inpath '/Users/dailiang/Documents/Code/StudyBigData/HadoopSpark/Project02_03_Traffic_Teach/src/main/resources/monitor_flow_action' into table monitor_flow_action");

//在hive中创建monitor_camera_info表
hiveContext.sql("DROP TABLE IF EXISTS monitor_camera_info"); 
hiveContext.sql("CREATE TABLE IF NOT EXISTS monitor_camera_info (monitor_id STRING, camera_id STRING) row format delimited fields terminated by '\t'");  
hiveContext.sql("LOAD DATA "
		+ "LOCAL INPATH '/Users/dailiang/Documents/Code/StudyBigData/HadoopSpark/Project02_03_Traffic_Teach/src/main/resources/monitor_camera_info'"
		+ "INTO TABLE monitor_camera_info");

System.out.println("========data2hive finish========");
sc.stop();
```

方法二：使用hive -f createHiveTab.sql
```
CREATE DATABASE IF NOT EXISTS traffic;

CREATE TABLE IF NOT EXISTS traffic.monitor_flow_action(  
`date` string ,  
monitor_id string ,  
camera_id string ,  
car string ,  
action_time string ,
speed string  ,
road_id string,
area_id string
)  
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' ; 

load data local inpath '/Users/dailiang/Documents/Code/StudyBigData/HadoopSpark/Project02_03_Traffic_Teach/src/main/resources/monitor_flow_action' into table traffic.monitor_flow_action; 

CREATE TABLE IF NOT EXISTS traffic.monitor_camera_info(  
monitor_id string ,  
camera_id string 
)  
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' ; 

load data local inpath '/Users/dailiang/Documents/Code/StudyBigData/HadoopSpark/Project02_03_Traffic_Teach/src/main/resources/monitor_camera_info' into table traffic.monitor_camera_info; 

```
# 3.Spark
## 3.1 Spark参数配置顺序
```
1.直接设置SparkConf
2.spark-submit or spark-shell提交任务时参数
3.spark-defaults.conf文件中配置
```
## 3.2 Spark SQL中SparkSession的使用

```
val path = args(0)
val spark = SparkSession.builder().appName("SparkSessionTest").master("local[2]").getOrCreate()
val people = spark.read.json(path)
people.show()
spark.stop()
```
## 3.3 报错处理
==需要拷贝hive.xml到resource目录下==
```
spark读取Hive出错： Database 'traffic' not found

加上enableHiveSupport()即可，否则默认不读hive-site.xml。

val spark = SparkSession.builder().master("local[2]").appName("HiveJoinMySql").enableHiveSupport().getOrCreate()

只能创建一个SparkContext
SparkContext sparkContext = spark.sparkContext();

```
## 3.4 kafka使用
### 3.4.1 kafka shell使用 
```
1.启动服务
bin/zookeeper-server-start.sh config/zookeeper.properties &

2.创建 topic
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic RoadRealTimeLog

3.通过list命令查看创建的topic
 bin/kafka-topics.sh --list --zookeeper localhost:2181
 
4.运行producer生产消息
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 

5.启动consumer消费消息
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic RoadRealTimeLog

6.删除topic里面的数据
    1）删除kafka相关的数据目录
        rm -r /var/kafka/log/tmp/test*
    2）删除kafka topic
        ./kafka-topics.sh --zookeeper localhost:2181 --delete --topic kfk
    3）删除zookeeper相关的路径
        （1）登录zookeeper客户端：命令：./bin/zookeeper-client
        （2）找到topic所在的目录：ls /brokers/topics
        （3）找到要删除的topic，执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。
```
### 3.4.2 kafka java 生产者使用 
```
Properties kafkaProps = new Properties();
kafkaProps.put("bootstrap.servers","localhost:9092");
kafkaProps.put("key.serializer",
		"org.apache.kafka.common.serialization.StringSerializer");
kafkaProps.put("value.serializer",
		"org.apache.kafka.common.serialization.StringSerializer") ;

while(true) {	
	producer.send(new ProducerRecord<String, String>("RoadRealTimeLog","message content");
}
				
```
### 3.4.2 kafka spark streaming 消费者者使用 
```
Map<String, Object> kafkaParams = new HashMap<>();
kafkaParams.put("bootstrap.servers", "localhost:9092,anotherhost:9092");
kafkaParams.put("key.deserializer", StringDeserializer.class);
kafkaParams.put("value.deserializer", StringDeserializer.class);
kafkaParams.put("group.id", "use_a_separate_group_id_for_each_stream");
kafkaParams.put("auto.offset.reset", "latest");
kafkaParams.put("enable.auto.commit", false);

Collection<String> topics = Arrays.asList("topicA", "topicB");

JavaInputDStream<ConsumerRecord<String, String>> stream =
  KafkaUtils.createDirectStream(
    streamingContext,
    LocationStrategies.PreferConsistent(),
    ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams)
  );

stream.mapToPair(record -> new Tuple2<>(record.key(), record.value()));
```
## 3.5 修改广播变量的值
```
/**
 * 读取文件，生成list，并广播
 * 若想改变广播变量的值，则只需要修改readFile的文件内容
 * @param args
 */
 // 构建Spark运行时的环境参数
SparkConf sparkConf = new SparkConf().setAppName("Sample car")
        .setMaster("local");

SparkSession spark = SparkSession
        .builder()
        .config(sparkConf)
        .enableHiveSupport()
        .getOrCreate();

SparkContext sparkContext = spark.sparkContext();
JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());

String path = "";
List<String> blackCars = readFile(path);
final Broadcast<List<String>> broadcast = sc.broadcast(blackCars);
```
## 3.6 开窗函数
```
开窗函数:row_number() over (partition by xxx order by xxx ) rank

select area_name,road_id,car_count,monitor_infos, row_number() over (partition by area_name order by car_count desc ) rank from tmp_area_road_flow_count

select
area_name,road_id,car_count,monitor_infos, row_number() over (partition by area_id order by carCount desc ) rank where rank <=3
```
## 3.7 SparkSQl中UDF
```
// 注册自定义函数
spark.udf().register("concat_String_string", new ConcatStringStringUDF(), DataTypes.StringType);
spark.udf().register("random_prefix", new RandomPrefixUDF(), DataTypes.StringType);
spark.udf().register("remove_random_prefix", new RemoveRandomPrefixUDF(), DataTypes.StringType);
spark.udf().register("group_concat_distinct",new GroupConcatDistinctUDAF());


/**
 * 将两个字段拼接起来（使用指定的分隔符）
 * @author Administrator
 *
 */
public class ConcatStringStringUDF implements UDF3<String, String, String, String> {
	private static final long serialVersionUID = 1L;
	
	@Override
	public String call(String area_name, String road_id, String split) throws Exception {
		return String.valueOf(area_name) + split + road_id;
	}

}

public class RandomPrefixUDF implements UDF2<String, Integer, String>{
	private static final long serialVersionUID = 1L;

	@Override
	public String call(String area_name_road_id, Integer ranNum) throws Exception {
		Random random = new Random();
		int prefix = random.nextInt(ranNum);
		return prefix+"_"+area_name_road_id;
	}

}

public class RemoveRandomPrefixUDF implements UDF1<String, String> {
	private static final long serialVersionUID = 1L;

	@Override
	//1_海淀区：建材城西路
	public String call(String val) throws Exception {
		return val.split("_")[1];
	}

}

/**
 * 组内拼接去重函数（group_concat_distinct()）
 * 
 * 技术点：自定义UDAF聚合函数
 * 
 * @author Administrator
 *
 */
public class GroupConcatDistinctUDAF extends UserDefinedAggregateFunction {

	private static final long serialVersionUID = -2510776241322950505L;
	
	// 指定输入数据的字段与类型
	private StructType inputSchema = DataTypes.createStructType(Arrays.asList(
			DataTypes.createStructField("carInfo", DataTypes.StringType, true)));  
	// 指定缓冲数据的字段与类型
	private StructType bufferSchema = DataTypes.createStructType(Arrays.asList(
			DataTypes.createStructField("bufferInfo", DataTypes.StringType, true)));  
	// 指定返回类型
	private DataType dataType = DataTypes.StringType;
	// 指定是否是确定性的
	private boolean deterministic = true;
	
	/**
	 * 输入数据的类型
	 */
	@Override
	public StructType inputSchema() {
		return inputSchema;
	}
	
	/**
	 * 聚合操作的数据类型
	 */
	@Override
	public StructType bufferSchema() {
		return bufferSchema;
	}

	@Override
	public boolean deterministic() {
		return deterministic;
	}
	
	/**
	 * 初始化
	 * 可以认为是，你自己在内部指定一个初始的值
	 */
	@Override
	public void initialize(MutableAggregationBuffer buffer) {
		buffer.update(0, "");  
	}
	
	/**
	 * 更新
	 * 可以认为是，一个一个地将组内的字段值传递进来
	 * 实现拼接的逻辑
	 */
	@Override
	public void update(MutableAggregationBuffer buffer, Row input) {
		// 缓冲中的已经拼接过的monitor信息小字符串
		String bufferMonitorInfo = buffer.getString(0);//|0001=1000|0002=2000|0003=3000
		// 刚刚传递进来的某个车辆信息
		String inputMonitorInfo = input.getString(0);
		
		String[] split = inputMonitorInfo.split("\\|");
		String monitorId = "";
		int addNum = 1;
		for (String string : split) {
			if(string.indexOf("=") != -1){
				monitorId = string.split("=")[0];
				addNum = Integer.parseInt(string.split("=")[1]);
			}else{
				monitorId = string;
			}
			String oldVS = StringUtils.getFieldFromConcatString(bufferMonitorInfo, "\\|", monitorId);
			if(oldVS == null) {
				bufferMonitorInfo += "|"+monitorId+"="+addNum;
			}else{
				bufferMonitorInfo = StringUtils.setFieldInConcatString(bufferMonitorInfo, "\\|", monitorId, Integer.parseInt(oldVS)+addNum+"");
			}
			buffer.update(0, bufferMonitorInfo);  
		}
	}
	
	/**
	 * 合并
	 * update操作，可能是针对一个分组内的部分数据，在某个节点上发生的
	 * 但是可能一个分组内的数据，会分布在多个节点上处理
	 * 此时就要用merge操作，将各个节点上分布式拼接好的串，合并起来
	 * merge1:|0001=100|0002=20|0003=4
	 * merge2:|0001=200|0002=30|0003=3
	 * 
	 * 海淀区 建材城西路1
	 * 海淀区 建材城西路2
	 */
	@Override
	public void merge(MutableAggregationBuffer buffer1, Row buffer2) {
		//缓存中的monitor信息这个大字符串
		String bufferMonitorInfo1 = buffer1.getString(0);
		//传进来          
		String bufferMonitorInfo2 = buffer2.getString(0);

        // 等于是把buffer2里面的数据都拆开来更新
		for(String monitorInfo : bufferMonitorInfo2.split("\\|")) {
			/**
			 * monitor_id1 100
			 * monitor_id2 88
			 * 
			 * 
			 */
			Map<String, String> map = StringUtils.getKeyValuesFromConcatString(monitorInfo, "\\|");
			for (Entry<String, String> entry : map.entrySet()) {
				String monitor = entry.getKey();
				int carCount = Integer.parseInt(entry.getValue());
				String oldVS = StringUtils.getFieldFromConcatString(bufferMonitorInfo1, "\\|", monitor);
				//当没有获取到本次monitor对应的值时
				if(oldVS == null) {
					if("".equals(bufferMonitorInfo1)) {
						//当第一次聚合的时候，没有初始的传进来的bufferMonitorInfo1，默认为""
						bufferMonitorInfo1 += monitor + "=" + carCount;
					} else {
						//当上一次传进来的字符串不包含本次的monitor时，就拼上
						bufferMonitorInfo1 += "|" + monitor + "=" + carCount;
					}
	 			}else{
	 				int oldVal = Integer.valueOf(oldVS);
	 				oldVal += carCount;
	 				bufferMonitorInfo1 = StringUtils.setFieldInConcatString(bufferMonitorInfo1, "\\|", monitor, oldVal+"");
	 			}
				buffer1.update(0, bufferMonitorInfo1);  
			}
		}
	}
	
	@Override
	public DataType dataType() {
		return dataType;
	}
	
	/**
	 * evaluate方法返回数据的类型要和dateType的类型一致，不一致就会报错
	 */
	@Override
	public Object evaluate(Row row) {  
		return row.getString(0);  
	}

}

```


# 4.项目问题
## 4.1 java代码中如何执行liunx脚本
```
String cmd = "ssh /root/test.sh " + args[0] + " " + args[1];
Process proc = Runtime.getRuntime().exec(cmd);

BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(proc.getInputStream()));
String flag;
while((flag = bufferedReader.readLine())!=null){
    System.out.print(flag)
}
bufferedReader.close();
proc.waitFor();
```
## 4.2 工具类

```
DateUtils：日期时间工具类
    boolean before(String time1, String time2)：判断一个时间是否在另一个时间之前
    boolean after(String time1, String time2：判断一个时间是否在另一个时间之后
    int minus(String time1, String time2)：计算时间差值（单位为秒）
    String getDateHour(String datetime)：获取年月日和小时【结果（yyyy-MM-dd_HH）】
    String getTodayDate()：获取当天日期（yyyy-MM-dd）
    String getYesterdayDate()：获取昨天的日期（yyyy-MM-dd）
    String formatDate(Date date)：格式化日期（yyyy-MM-dd）
    String formatTime(Date date)：格式化时间（yyyy-MM-dd HH:mm:ss）
    Date parseTime(String time) ：解析时间字符串
    String formatDateKey(Date date)：格式化日期key【yyyyMMdd】
    Date parseDateKey(String datekey：格式化日期key	(yyyyMMdd)
    String formatTimeMinute(Date date)：格式化时间，保留到分钟级别【yyyyMMddHHmm】
    String getRangeTime(String dateTime)：yyyy-MM-dd HH:mm:ss
    
    
NumberUtils：数字格工具类
    double formatDouble(double num, int scale)：格式化小数，@param num 字符串，@param scale 四舍五入的位数


StringUtils：字符串工具类
    boolean isEmpty(String str)：判断字符串是否为空
    boolean isNotEmpty(String str)：判断字符串是否不为空
    tring trimComma(String str)：截断字符串两侧的逗号
    static String fulfuill(String str)：补全两位数字
    String fulfuill(int num,String str)：补全num位数字，前面补0，使字符串的长度为num位
    String getFieldFromConcatString(String str,String delimiter, String field)：从拼接的字符串中提取字段，@param str 字符串，@param delimiter 分隔符，@param field 字段
    String setFieldInConcatString(String str,String delimiter, String field, String newFieldValue)：从拼接的字符串中给字段设置值，@param str 字符串，@param delimiter 分隔符，@param field 字段名，@param newFieldValue 新的field值
    Map<String, String> getKeyValuesFromConcatString(String str,String delimiter)：给定字符串和分隔符，返回一个K,V map
    Integer convertStringtoInt(String str)：String 字符串转Integer数字
    

ParamUtils：参数工具类
    Long getTaskIdFromArgs(String[] args, String taskType): 从命令行参数中提取任务id【@param args 命令行参数, @param taskType 参数类型(任务id对应的值是Long类型才可以)，对应my.properties中的key】
    String getParam(JSONObject jsonObject, String field)：从JSON对象中提取参数


SparkUtils：Spark工具类
    void setMaster(SparkConf conf)：根据当前是否本地测试的配置，决定 如何设置SparkConf的master
    SQLContext getSQLContext(JavaSparkContext sc)：获取SQLContext,如果spark.local设置为true，那么就创建SQLContext；否则，创建HiveContext
    void mockData(JavaSparkContext sc, SQLContext sqlContext)：生成模拟数据，如果spark.local配置设置为true，则生成模拟数据；否则不生成
    JavaRDD<Row> getCameraRDDByDateRange(SQLContext sqlContext, JSONObject taskParamsJsonObject)：获取指定日期范围内的卡口信息
    JavaRDD<Row> getCameraRDDByDateRangeAndCars(SQLContext sqlContext, JSONObject taskParamsJsonObject)：获取指定日期内出现指定车辆的卡扣信息
    JavaRDD<Row> getCameraRDDByDateRangeAndArea(SQLContext sqlContext, JSONObject taskParamsJsonObject,String a)：获取指定日期范围和指定区域范围内的卡口信息
```




